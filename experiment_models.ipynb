{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import laspy\n",
    "import numpy as np\n",
    "\n",
    "def load_and_combine_laz_files(file_paths, subsample_ratio=0.1):\n",
    "    combined_data = {\n",
    "        \"points\": [],\n",
    "        \"intensity\": [],\n",
    "        \"classification\": [],\n",
    "        \"return_number\": [],\n",
    "        \"number_of_returns\": [],\n",
    "    }\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        las = laspy.read(file_path)\n",
    "        num_points = len(las.x)\n",
    "        sample_size = int(num_points * subsample_ratio)\n",
    "        indices = np.random.choice(num_points, sample_size, replace=False)\n",
    "\n",
    "        combined_data[\"points\"].append(\n",
    "            np.vstack((las.x[indices], las.y[indices], las.z[indices])).T\n",
    "        )\n",
    "        combined_data[\"intensity\"].append(las.intensity[indices])\n",
    "        combined_data[\"classification\"].append(las.classification[indices])\n",
    "        combined_data[\"return_number\"].append(las.return_number[indices])\n",
    "        combined_data[\"number_of_returns\"].append(las.number_of_returns[indices])\n",
    "\n",
    "    for key in combined_data:\n",
    "        combined_data[key] = np.concatenate(combined_data[key])\n",
    "\n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_and_pca(data, n_components=3):\n",
    "    # Ensure points are n x 3\n",
    "    if data[\"points\"].shape[0] == 3:\n",
    "        data[\"points\"] = data[\"points\"].T\n",
    "\n",
    "    # Normalize intensity and height\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_intensity = scaler.fit_transform(data[\"intensity\"].reshape(-1, 1))\n",
    "    normalized_height = scaler.fit_transform(data[\"points\"][:, 2].reshape(-1, 1))\n",
    "\n",
    "    # Combine features: x, y, z, intensity, height\n",
    "    features = np.hstack((data[\"points\"], normalized_intensity, normalized_height))\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(features)\n",
    "\n",
    "    return reduced_features, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "def rasterize_points(data, grid_size=(512, 512), method='nearest'):\n",
    "    x, y, z = data[\"points\"].T\n",
    "    intensity = data[\"intensity\"]\n",
    "    classification = data[\"classification\"]\n",
    "\n",
    "    grid_x, grid_y = np.linspace(x.min(), x.max(), grid_size[0]), np.linspace(y.min(), y.max(), grid_size[1])\n",
    "    grid_x, grid_y = np.meshgrid(grid_x, grid_y)\n",
    "\n",
    "    intensity_map = griddata((x, y), intensity, (grid_x, grid_y), method=method)\n",
    "    classification_map = griddata((x, y), classification, (grid_x, grid_y), method=method)\n",
    "\n",
    "    return intensity_map, classification_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_train_test_split(data, test_ratio=0.3):\n",
    "    # Ensure data['points'] is a NumPy array\n",
    "    if not isinstance(data[\"points\"], np.ndarray):\n",
    "        data[\"points\"] = np.array(data[\"points\"])\n",
    "\n",
    "    # Unpack x, y, z coordinates\n",
    "    x, y, z = data[\"points\"].T\n",
    "\n",
    "    # Split into spatial regions\n",
    "    mask = np.random.rand(x.shape[0]) < test_ratio\n",
    "\n",
    "    # Base train and test data\n",
    "    train_data = {\n",
    "        \"points\": data[\"points\"][~mask],\n",
    "        \"intensity\": data[\"intensity\"][~mask],\n",
    "        \"classification\": data[\"classification\"][~mask],\n",
    "    }\n",
    "    test_data = {\n",
    "        \"points\": data[\"points\"][mask],\n",
    "        \"intensity\": data[\"intensity\"][mask],\n",
    "        \"classification\": data[\"classification\"][mask],\n",
    "    }\n",
    "\n",
    "    # Include PCA features if present\n",
    "    if \"pca_features\" in data:\n",
    "        train_data[\"pca_features\"] = data[\"pca_features\"][~mask]\n",
    "        test_data[\"pca_features\"] = data[\"pca_features\"][mask]\n",
    "\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(data, patch_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Extract patches from 2D rasterized data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (ndarray): 2D rasterized data.\n",
    "        patch_size (tuple): Height and width of each patch.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Array of patches.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    h, w = data.shape\n",
    "    patch_h, patch_w = patch_size\n",
    "    for i in range(0, h - patch_h + 1, patch_h):\n",
    "        for j in range(0, w - patch_w + 1, patch_w):\n",
    "            patches.append(data[i:i+patch_h, j:j+patch_w])\n",
    "    return np.array(patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LAZ files: 3\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m X_val \u001b[38;5;241m=\u001b[39m unet_input_patches[num_train:]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Use patches themselves as both input and target for unsupervised learning\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43munet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Target is the same as input for reconstruction\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Step 6: Evaluate and save\u001b[39;00m\n\u001b[1;32m     96\u001b[0m unet_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munet_unsupervised_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_clone/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_clone/lib/python3.9/site-packages/keras/engine/training.py:1395\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1393\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected result of `train_function` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1396\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(Empty logs). Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1397\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1398\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1399\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1400\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1401\u001b[0m epoch_logs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(logs)\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;66;03m# Run validation.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras_unet_collection.models import unet_2d\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# Step 1: Generate pseudo-labels using clustering\n",
    "def create_pseudo_labels(data, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Generate pseudo-labels using KMeans clustering on PCA features.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(data[\"pca_features\"])\n",
    "    return labels\n",
    "\n",
    "# Step 2: Prepare the U-Net model\n",
    "def build_unet(input_shape, n_clusters):\n",
    "    \"\"\"\n",
    "    Build a U-Net model for unsupervised learning.\n",
    "    \"\"\"\n",
    "    model = unet_2d(\n",
    "        input_shape,\n",
    "        filter_num=[64, 128, 256, 512],\n",
    "        n_labels=n_clusters,\n",
    "        stack_num_down=2,\n",
    "        stack_num_up=2,\n",
    "        activation=\"ReLU\",\n",
    "        output_activation=\"Softmax\",\n",
    "        batch_norm=True,\n",
    "    )\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())\n",
    "    return model\n",
    "\n",
    "# Step 3: Prepare data for U-Net\n",
    "def prepare_data_for_unet(data, grid_size=(128, 128), patch_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Rasterize the point cloud and extract patches.\n",
    "    \"\"\"\n",
    "    intensity_map, _ = rasterize_points(data, grid_size=grid_size)\n",
    "    patches = extract_patches(intensity_map, patch_size=patch_size)\n",
    "    patches = patches[..., np.newaxis]  # Add channel dimension\n",
    "    return patches\n",
    "\n",
    "base_dir = r\"data/\"\n",
    "\n",
    "laz_data_path_list = [\n",
    "    os.path.join(root, file)\n",
    "    for root, _, files in os.walk(base_dir)\n",
    "    for file in files\n",
    "    if file.endswith(\".laz\")\n",
    "]\n",
    "print(\"Number of LAZ files:\", len(laz_data_path_list))\n",
    "\n",
    "\n",
    "# Combine and preprocess data\n",
    "combined_data = load_and_combine_laz_files(laz_data_path_list, subsample_ratio=0.5)\n",
    "\n",
    "# Normalize and apply PCA\n",
    "reduced_features, pca_model = normalize_and_pca(combined_data)\n",
    "\n",
    "# Add PCA features to the combined data\n",
    "combined_data[\"pca_features\"] = reduced_features\n",
    "\n",
    "# Create pseudo-labels\n",
    "pseudo_labels = create_pseudo_labels(combined_data)\n",
    "\n",
    "# Add pseudo-labels to combined data\n",
    "combined_data[\"pseudo_labels\"] = pseudo_labels\n",
    "\n",
    "# Rasterize and prepare patches\n",
    "unet_input_patches = prepare_data_for_unet(combined_data)\n",
    "\n",
    "# Build U-Net\n",
    "input_shape = (128, 128, 1)  # Adjust channels if needed\n",
    "n_clusters = len(set(pseudo_labels))\n",
    "unet_model = build_unet(input_shape, n_clusters)\n",
    "\n",
    "# Step 5: Train the U-Net\n",
    "# Split patches into train/test if needed\n",
    "train_ratio = 0.8\n",
    "num_train = int(len(unet_input_patches) * train_ratio)\n",
    "X_train = unet_input_patches[:num_train]\n",
    "X_val = unet_input_patches[num_train:]\n",
    "\n",
    "# Use patches themselves as both input and target for unsupervised learning\n",
    "unet_model.fit(\n",
    "    X_train,\n",
    "    X_train,  # Target is the same as input for reconstruction\n",
    "    validation_data=(X_val, X_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "# Step 6: Evaluate and save\n",
    "unet_model.save(\"unet_unsupervised_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches extracted: 1\n"
     ]
    }
   ],
   "source": [
    "unet_input_patches = prepare_data_for_unet(combined_data)\n",
    "print(\"Number of patches extracted:\", len(unet_input_patches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches: 1\n",
      "Number of training patches: 0\n",
      "Number of validation patches: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Total patches:\", len(unet_input_patches))\n",
    "num_train = int(len(unet_input_patches) * train_ratio)\n",
    "print(\"Number of training patches:\", num_train)\n",
    "print(\"Number of validation patches:\", len(unet_input_patches) - num_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points shape: (30470117, 3)\n",
      "Intensity shape: (30470117,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Points shape:\", combined_data[\"points\"].shape)\n",
    "print(\"Intensity shape:\", combined_data[\"intensity\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_unet_prediction(unet_model, patches, num_examples=5):\n",
    "    \"\"\"\n",
    "    Visualize input patches and their corresponding U-Net predictions.\n",
    "    \n",
    "    Parameters:\n",
    "        unet_model: Trained U-Net model.\n",
    "        patches: Input patches for prediction.\n",
    "        num_examples: Number of examples to visualize.\n",
    "    \"\"\"\n",
    "    # Select a random subset of patches to visualize\n",
    "    indices = np.random.choice(len(patches), num_examples, replace=False)\n",
    "    selected_patches = patches[indices]\n",
    "    \n",
    "    # Predict using the U-Net model\n",
    "    predictions = unet_model.predict(selected_patches)\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, axes = plt.subplots(num_examples, 2, figsize=(8, 4 * num_examples))\n",
    "    for i, (input_patch, predicted_patch) in enumerate(zip(selected_patches, predictions)):\n",
    "        if num_examples > 1:\n",
    "            ax_input, ax_pred = axes[i]\n",
    "        else:\n",
    "            ax_input, ax_pred = axes\n",
    "        \n",
    "        # Input patch\n",
    "        ax_input.imshow(input_patch.squeeze(), cmap=\"gray\")\n",
    "        ax_input.set_title(\"Input Patch\")\n",
    "        ax_input.axis(\"off\")\n",
    "        \n",
    "        # Predicted patch\n",
    "        ax_pred.imshow(predicted_patch.squeeze(), cmap=\"gray\")\n",
    "        ax_pred.set_title(\"Predicted Patch\")\n",
    "        ax_pred.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_unet_prediction(unet_model, unet_input_patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
